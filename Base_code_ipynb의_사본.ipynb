{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Base_code.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "c823e32c24d06b3518059b5f5e11ccae729689482027ea5186dc849db7650ef7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit ('torch': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jykim-rust/CSIC-/blob/main/Base_code_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyUaAVxDPQHZ"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from google.colab import files\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4FsG6OcPQHe"
      },
      "source": [
        "파싱"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mujG_ApBPQHg"
      },
      "source": [
        "def parsing(path):#파싱을 진행하는 함수\n",
        "    with open(path,'r',encoding='utf-8') as f:\n",
        "        #파일을 읽어들이고 ['로그','로그',...] 이런식으로 로그를 구조화\n",
        "        train = []\n",
        "        log_data = \"\" # 하나의 로그에 대해 추출한 url path와 query, payload 값을 \" \"로 구분한 긴 문자열\n",
        "        para=\"\" # 하나의 로그에 대해 모든 내용을 담고 있는 문자열\n",
        "        while True:\n",
        "            l = f.readline() #한줄씩 읽어 옵니다\n",
        "            if not l:\n",
        "                break #파일을 전부 읽으면 읽기를 중단합니다.\n",
        "\n",
        "            if l != \"\\n\":\n",
        "                para +=l\n",
        "                if l[:4] == 'GET ' or l[:4] == 'PUT ':\n",
        "                    # GET method나 PUT method일 경우 url의 path와 query 추출\n",
        "                    url = l[4:len(l)-10]\n",
        "                    # url 은 method type과 HTTP/1.1을 뺀 중간 부분\n",
        "                    url_path = url[21:]\n",
        "                    # url에서 domain인 http://localhost:8080 제거 \n",
        "                    query_start = url_path.find('?')\n",
        "                    if (query_start != -1):\n",
        "                        # query 문이 있을 경우 path와 query들을 분리\n",
        "                        log_data += url_path[:query_start] + \" \"\n",
        "                        log_data += \" \".join(url_path[query_start+1:len(url)].split(\"&\")) + \" \"\n",
        "                    else:\n",
        "                        # query 문이 없을 경우 path 만 저장\n",
        "                        log_data += url_path + \" \"   \n",
        "\n",
        "                elif l[:5] == 'POST ':\n",
        "                    # POST method일 경우 url의 path 추출\n",
        "                    url = l[5:len(l)-10]\n",
        "                    url_path = url[21:]\n",
        "                    log_data += url_path + \" \"\n",
        "                                          \n",
        "            else:\n",
        "                if para!='':\n",
        "                    if para[:4]=='POST' or para[:3] == 'PUT': #Method가 POST나 PUT일 경우 예외적으로 바디까지 가져옵니다.\n",
        "                        log_data += \" \".join(f.readline().strip().split(\"&\")) + \" \"\n",
        "                    # print(log_data)\n",
        "                    train.append(log_data)\n",
        "                    para=\"\"\n",
        "                    log_data =\"\"\n",
        "                    # print(\"=============================================\")\n",
        "    return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbo4XsESPQHh"
      },
      "source": [
        "def dataset(path,mod='train'): #데이터셋을 생성합니다. 파싱한 데이터와 라벨을 생성합니다 \n",
        "    x = parsing(f'{path}norm_{mod}.txt') # mod에 따라 train을 가져올지 test 데이터를 가져올지 결정됩니다.\n",
        "    y = [0]*len(x)\n",
        "    x += parsing(f'{path}anomal_{mod}.txt')\n",
        "    y += [1]*(len(x)-len(y))\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSFNVOTMPQHi"
      },
      "source": [
        "def vectorize(train_x,test_x): #문장을 벡터로 만듭니다 해당 코드에서는 기본적인 tf idf를 사용하고 있습니다.\n",
        "    tf = TfidfVectorizer(min_df=0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(3,3))\n",
        "    # 설명 필요\n",
        "    tf = tf.fit(train_x)\n",
        "    train_vec = tf.transform(train_x)\n",
        "    test_vec = tf.transform(test_x)\n",
        "    return train_vec,test_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqiXaTynPQHi"
      },
      "source": [
        "def train(train_vec,train_y): #랜덤 포레스트로 훈련 시킵니다. 모델을 바꾸고 싶다면 이 함수를 변경해야 합니다.\n",
        "    linear_svm = LinearSVC()\n",
        "    linear_svm.fit(train_vec, train_y)\n",
        "    return linear_svm #99프로 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox8t2ROpPQHj"
      },
      "source": [
        "def test(test_y,test_vec, svm): #입렵 받은 테스트와 모델로 테스트를 실시합니다\n",
        "    pred = svm.predict(test_vec)\n",
        "    print(accuracy_score(test_y,pred))\n",
        "    print(f1_score(test_y,pred))\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "9LPMi06lPQHk",
        "outputId": "3eaeb156-20ef-4604-a122-db199abd67b3"
      },
      "source": [
        "############### 실행 코드 #######################\n",
        "################################################\n",
        "train_x, train_y = dataset('./','train')\n",
        "test_x, test_y =  dataset('./','test')\n",
        "train_vec, test_vec = vectorize(train_x, test_x)\n",
        "svm = train(train_vec, train_y)\n",
        "pred = test(test_y,test_vec, svm)\n",
        "\n",
        "# ########################################################\n",
        "tf = TfidfVectorizer()\n",
        "tf = tf.fit(train_x)\n",
        "\n",
        "print(len(tf.vocabulary_)) # 고유한 단어가 대략 8만개가 나옵니다\n",
        "\n",
        "print(tf.transform(train_x)[0]) #로그 하나당 약 8만차원이 나옵니다\n",
        "                            # 필요없는 문장 때문에 단어의 개수가 많이 진것일  수도 있으니 데이터를 분석하여 필요한 부분만 사용하는것이 좋습니다\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d7a41f63f8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############### 실행 코드 #######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3fb85f900d0a>\u001b[0m in \u001b[0;36mdataset\u001b[0;34m(path, mod)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#데이터셋을 생성합니다. 파싱한 데이터와 라벨을 생성합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}norm_{mod}.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# mod에 따라 train을 가져올지 test 데이터를 가져올지 결정됩니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}anomal_{mod}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4eeeb040d8a2>\u001b[0m in \u001b[0;36mparsing\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#파싱을 진행하는 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;31m#파일을 읽어들이고 ['로그','로그',...] 이런식으로 로그를 구조화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;31m# 하나의 로그에 대해 추출한 url path와 query, payload 값을 \" \"로 구분한 긴 문자열\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './norm_train.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTc76BEtid3L"
      },
      "source": [
        "조교님 url에서의 path와 query 값과 payload 값만 추출해서 진행하던 중 \n",
        "RandomForestClassifier, LogisticRegression, LinearSVC, DecisionTreeClassifier 등 \n",
        "여러 모델을 써봤는데도 여전히 최대 86프로 정도의 정확률을 보입니다.\n",
        "파라미터 값을 바꿔보거나 scaler를 사용해봐도 오히려 더 떨어지거나 하는 중입니다.\n",
        "자료 조사를 하던 중 대부분의 사람들이 train set에서  train_test_split 을 사용해 train data와 validation data로 구분해서 학습과 검증을 진행시키는 경우에는 98~99프로의 정확률을 내는 것을 발견했습니다.\n",
        "1. 혹시 train 만을 이용해서 학습시키고 test set을 검증했을 때 85프로 정도가 나오는 게 맞는 것인지, 아니면 오버피팅 때문에 잘못 나온 것인지 궁금합니다.\n",
        "2. 오버피팅 때문이라면 다른 자료들처럼 train과 validation으로 구분해서 진행했을 때 train set의 validation data를 검증한 걸로 어떻게 test set에 대해 더 좋은 정확률이 나오도록 조정할 수 있는지 모르겠습니다. \n",
        "3. 또한 과제에서 요구하는 정확도가 어느 정도인지 궁금합니다.\n",
        "\n",
        "\n",
        "- 일단 payload 수정하니까 99프로, 98프로 나옴... \n",
        "답장 오면 payload 값 때문이었다고 하고 2번 질문 한 거 해야하는지 물어보기 \n"
      ]
    }
  ]
}